#!/usr/bin/env python3
"""
æ•°æ®åŒæ­¥å®ˆæŠ¤è¿›ç¨‹

æ¯10åˆ†é’Ÿè‡ªåŠ¨åŒæ­¥è§‚å¯Ÿåˆ—è¡¨æ•°æ®åˆ°æœ¬åœ°æ•°æ®åº“ã€‚

âš ï¸  æ•°æ®å»¶è¿Ÿè¯´æ˜ï¼š
    æœ¬ç³»ç»Ÿä½¿ç”¨ ThetaData v3 å…è´¹è®¡åˆ’ï¼ˆvenue='utp_cta'ï¼‰ï¼Œæ•°æ®ç›¸å¯¹å®ç›˜æœ‰ 15 åˆ†é’Ÿå»¶è¿Ÿã€‚
    è¿™æ˜¯æ­£å¸¸ç°è±¡ï¼Œä¸å½±å“å†å²æ•°æ®åˆ†æå’Œç­–ç•¥å›æµ‹ã€‚

ç‰¹æ€§ï¼š
- âœ… å¢é‡æ›´æ–°ï¼šåªè·å–æ–°æ•°æ®ï¼Œè‡ªåŠ¨å»é‡
- âœ… å¸‚åœºæ„ŸçŸ¥ï¼šåªåœ¨äº¤æ˜“æ—¶æ®µä¸»åŠ¨åŒæ­¥
- âœ… é”™è¯¯é‡è¯•ï¼šç½‘ç»œå¤±è´¥è‡ªåŠ¨é‡è¯•
- âœ… å®Œæ•´æ—¥å¿—ï¼šè®°å½•æ‰€æœ‰åŒæ­¥æ´»åŠ¨
- âœ… æ—¶åŒºæ„ŸçŸ¥ï¼šæ‰€æœ‰æ—¶é—´ä½¿ç”¨ç¾ä¸œæ—¶åŒº (ET/America/New_York)

ä½¿ç”¨æ–¹æ³•ï¼š
    # ç›´æ¥è¿è¡Œï¼ˆå‰å°ï¼‰
    python runtime/data_sync_daemon.py

    # åå°è¿è¡Œ
    nohup python runtime/data_sync_daemon.py > logs/data_sync.log 2>&1 &

    # ä½¿ç”¨ systemdï¼ˆæ¨èç”Ÿäº§ç¯å¢ƒï¼‰
    sudo systemctl start agentic-data-sync

    # ä½¿ç”¨ cronï¼ˆæ¯10åˆ†é’Ÿï¼‰
    */10 * * * * cd /path/to/agentic_trading && python runtime/data_sync_daemon.py --once
"""

import sys
from pathlib import Path

# Add project root to path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import time
import logging
import argparse
from datetime import datetime, timedelta
from typing import Dict, List

# Import skills
from skills import (
    sync_watchlist_incremental,
    process_snapshot_and_cache,
    get_data_freshness_report
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('logs/data_sync.log') if Path('logs').exists() else logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


def run_sync_cycle() -> Dict:
    """
    æ‰§è¡Œä¸€æ¬¡å®Œæ•´çš„æ•°æ®åŒæ­¥å‘¨æœŸ

    Returns:
        åŒæ­¥ç»Ÿè®¡ä¿¡æ¯
    """
    logger.info("=" * 70)
    logger.info("ğŸ“Š Starting Data Sync Cycle")
    logger.info("=" * 70)

    cycle_start = time.time()

    # 1. è°ƒç”¨å¢é‡åŒæ­¥æŠ€èƒ½ï¼ˆä¼šè‡ªåŠ¨æ£€æŸ¥å¸‚åœºçŠ¶æ€ï¼‰
    sync_result = sync_watchlist_incremental(skip_if_market_closed=True)

    # 2. è§£æå¸‚åœºçŠ¶æ€
    market_status = sync_result.get('market_status', {})
    logger.info(f"ğŸ•’ Timestamp: {market_status.get('timestamp', 'N/A')}")
    logger.info(f"ğŸ“ˆ Market Status: {market_status.get('session', 'UNKNOWN')}")
    logger.info(f"ğŸ”“ Market Open: {'âœ… YES' if market_status.get('market_open') else 'âŒ NO'}")

    if market_status.get('next_market_open'):
        logger.info(f"â° Next Open: {market_status['next_market_open']}")

    logger.info(f"â±ï¸  Data Delay: ğŸ“ 15 minutes (ThetaData v3 å…è´¹è®¡åˆ’)")

    # 3. æ£€æŸ¥æ˜¯å¦æˆåŠŸåŒæ­¥
    if not sync_result.get('success'):
        # å¸‚åœºå…³é—­æˆ–å…¶ä»–é”™è¯¯
        errors = sync_result.get('errors', [])
        reason = errors[0] if errors else "Unknown reason"
        logger.info(f"â­ï¸  Skip Reason: {reason}")
        logger.info("=" * 70 + "\n")
        return {
            'synced': False,
            'reason': reason,
            'market_status': market_status,
            'stats': {
                'total': sync_result.get('total_symbols', 0),
                'success': 0,
                'failed': 0
            }
        }

    # 4. è¾“å‡ºåŒæ­¥ç»Ÿè®¡ä¿¡æ¯
    total_symbols = sync_result.get('total_symbols', 0)
    synced_count = sync_result.get('synced_count', 0)
    failed_count = sync_result.get('failed_count', 0)
    execution_time = sync_result.get('execution_time', 0)

    logger.info(f"\nğŸ“‹ Total Symbols: {total_symbols}")
    logger.info(f"âœ… Synced: {synced_count}")
    logger.info(f"âŒ Failed: {failed_count}")
    logger.info(f"â±ï¸  Execution Time: {execution_time:.2f}s")

    # 5. æ˜¾ç¤ºè¯¦ç»†ç»“æœï¼ˆå‰10ä¸ªï¼‰
    results = sync_result.get('results', [])
    if results:
        logger.info("\n" + "â”€" * 70)
        logger.info("Sample Results (first 10):")
        logger.info("â”€" * 70)
        for i, result in enumerate(results[:10], 1):
            symbol = result.get('symbol', 'UNKNOWN')
            status = result.get('status', 'unknown')

            if status == 'synced':
                bars_added = result.get('bars_added', 0)
                timestamp = result.get('timestamp', 'N/A')
                if bars_added > 0:
                    logger.info(f"  [{i}] âœ… {symbol}: New bar @ {timestamp}")
                else:
                    logger.info(f"  [{i}] â­ï¸  {symbol}: Duplicate (already in DB)")
            else:
                error = result.get('error', 'Unknown error')
                logger.info(f"  [{i}] âŒ {symbol}: {error}")

    # 6. æ˜¾ç¤ºé”™è¯¯ï¼ˆå¦‚æœæœ‰ï¼‰
    errors = sync_result.get('errors', [])
    if errors:
        logger.warning("\nâš ï¸  Errors:")
        for err in errors[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé”™è¯¯
            logger.warning(f"   - {err}")

    logger.info("\n" + "=" * 70)
    logger.info("ğŸ“Š Sync Cycle Complete")
    logger.info("=" * 70 + "\n")

    return {
        'synced': True,
        'stats': {
            'total': total_symbols,
            'success': synced_count,
            'failed': failed_count,
            'execution_time': execution_time
        },
        'market_status': market_status
    }


def run_continuous(interval_minutes: int = 10):
    """
    æŒç»­è¿è¡ŒåŒæ­¥å®ˆæŠ¤è¿›ç¨‹

    Args:
        interval_minutes: åŒæ­¥é—´éš”ï¼ˆåˆ†é’Ÿï¼‰
    """
    logger.info("ğŸš€ Agentic AlphaHive - Data Sync Daemon")
    logger.info("=" * 70)
    logger.info(f"â° Sync Interval: {interval_minutes} minutes")
    logger.info(f"ğŸ’¾ Database: data_lake/trades.db")
    logger.info(f"ğŸ›‘ Press Ctrl+C to stop")
    logger.info("=" * 70 + "\n")

    cycle_count = 0

    try:
        while True:
            cycle_count += 1
            logger.info(f"ğŸ”„ Cycle #{cycle_count}")

            # æ‰§è¡ŒåŒæ­¥
            result = run_sync_cycle()

            # è®¡ç®—ä¸‹æ¬¡åŒæ­¥æ—¶é—´
            wait_seconds = interval_minutes * 60
            next_sync = datetime.now() + timedelta(seconds=wait_seconds)

            logger.info(f"â³ Waiting {interval_minutes} minutes...")
            logger.info(f"â° Next sync: {next_sync.strftime('%Y-%m-%d %H:%M:%S')}\n")

            time.sleep(wait_seconds)

    except KeyboardInterrupt:
        logger.info("\n\nğŸ›‘ Shutdown signal received")
        logger.info(f"âœ… Completed {cycle_count} sync cycles")
        logger.info("ğŸ‘‹ Data Sync Daemon stopped\n")


def main():
    """ä¸»å…¥å£"""
    parser = argparse.ArgumentParser(
        description='å¢é‡å¸‚åœºæ•°æ®åŒæ­¥å®ˆæŠ¤è¿›ç¨‹',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # è¿è¡Œä¸€æ¬¡åé€€å‡º
  python runtime/data_sync_daemon.py --once

  # æŒç»­è¿è¡Œï¼ˆæ¯10åˆ†é’ŸåŒæ­¥ï¼‰
  python runtime/data_sync_daemon.py --interval 10

  # åå°è¿è¡Œ
  nohup python runtime/data_sync_daemon.py --interval 10 > logs/data_sync.log 2>&1 &

  # ä½¿ç”¨ cronï¼ˆæ¯10åˆ†é’Ÿï¼‰
  */10 * * * * cd /path/to/agentic_trading && python runtime/data_sync_daemon.py --once

æ³¨æ„ï¼š
  1. æ•°æ®æ¥æºï¼šThetaData v3 API (venue='utp_cta')
  2. æ•°æ®å»¶è¿Ÿï¼šç›¸å¯¹å®ç›˜æœ‰ 15 åˆ†é’Ÿå»¶è¿Ÿï¼ˆå…è´¹è®¡åˆ’é™åˆ¶ï¼‰
  3. æ—¶åŒºå¤„ç†ï¼šæ‰€æœ‰æ—¶é—´æˆ³ä½¿ç”¨ç¾ä¸œæ—¶åŒº (ET/America/New_York)
  4. å¸‚åœºæ„ŸçŸ¥ï¼šåªåœ¨äº¤æ˜“æ—¶æ®µä¸»åŠ¨åŒæ­¥æ•°æ®
        """
    )

    parser.add_argument(
        '--once',
        action='store_true',
        help='åªè¿è¡Œä¸€æ¬¡åŒæ­¥åé€€å‡º'
    )

    parser.add_argument(
        '--interval',
        type=int,
        default=10,
        help='æŒç»­æ¨¡å¼çš„åŒæ­¥é—´éš”ï¼ˆåˆ†é’Ÿï¼‰ï¼Œé»˜è®¤10åˆ†é’Ÿ'
    )

    args = parser.parse_args()

    # ç¡®ä¿ logs ç›®å½•å­˜åœ¨
    Path('logs').mkdir(exist_ok=True)

    if args.once:
        logger.info("Mode: Single Sync\n")
        result = run_sync_cycle()
        if result['synced']:
            logger.info(f"âœ… Sync completed: {result['stats']['success']}/{result['stats']['total']} symbols")
            sys.exit(0)
        else:
            logger.info(f"â­ï¸  Sync skipped: {result['reason']}")
            sys.exit(0)
    else:
        logger.info(f"Mode: Continuous (every {args.interval} minutes)\n")
        run_continuous(args.interval)


if __name__ == '__main__':
    main()
